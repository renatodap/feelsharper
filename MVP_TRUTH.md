# FeelSharper MVP - The Single Source of Truth
**Last Updated**: 2025-08-19
**Status**: IMPLEMENTING TRUE A+ VERSION

## ðŸŽ¯ MVP Core Requirements

### Essential Features (MUST HAVE)
1. **Food Logging** âœ… - Track what you eat
2. **Weight Tracking** âœ… - Monitor weight changes  
3. **Exercise Logging** âœ… - Record workouts

### Revolutionary Feature: Unified Natural Language Input
- **Single input box** that understands everything
- **No need to specify type** - AI figures out if it's food, weight, or exercise
- **Voice input** via Web Speech API
- **Photo input** with AI analysis
- **Smart auto-logging** when confidence is high (>80%)
- **Confirmation UI** for uncertain inputs (50-80% confidence)

## ðŸ—ï¸ Implementation Status

### âœ… COMPLETED
- OpenAI + Claude API integration
- Basic natural language parsing endpoint (`/api/ai/parse`)
- Pattern matching for weight, food, exercise
- Traditional pages (food/add, weight, etc.)
- Database schema (Supabase)
- Production build working

### ðŸš§ IMPLEMENTING NOW
- [x] UnifiedNaturalInput component with voice + photo
- [ ] Smart auto-logging with confidence levels
- [ ] Image parsing endpoint (`/api/ai/parse-image`)
- [ ] Activity logging endpoint (`/api/activities/log`)
- [ ] Integration on main dashboard
- [ ] Full testing of all input types

## ðŸ“± User Experience

### How It Works
1. **User opens app** â†’ Sees unified input box
2. **User speaks/types/photos** â†’ "weight 175" or "ran 5k" or photo of meal
3. **AI parses** â†’ Determines type and extracts data
4. **Smart logging**:
   - High confidence (>80%) â†’ Auto-logs
   - Medium confidence (50-80%) â†’ Shows confirmation
   - Low confidence (<50%) â†’ Asks for clarification
5. **Instant feedback** â†’ "âœ“ Weight logged: 175 lbs"

### Example Interactions
```
User: "175"
AI: [Confirms] "Log weight as 175 lbs?" [Yes/No]

User: "weight 175"  
AI: [Auto-logs] "âœ“ Weight logged: 175 lbs"

User: "Ran 5k in 25 minutes, feeling great"
AI: [Auto-logs] "âœ“ Exercise logged: 5k run, 25 minutes"

User: "Chicken salad for lunch"
AI: [Auto-logs] "âœ“ Food logged: Chicken salad (lunch)"

User: [Photos meal]
AI: [Analyzes] "âœ“ Food logged: Sandwich, apple, water"
```

## ðŸ”§ Technical Architecture

### Components
```
UnifiedNaturalInput.tsx    # Main input component
â”œâ”€â”€ Voice input (Web Speech API)
â”œâ”€â”€ Photo upload + preview
â”œâ”€â”€ Text input with auto-submit
â”œâ”€â”€ Confidence-based UI
â””â”€â”€ Auto-logging system

/api/ai/parse              # Natural language parsing
â”œâ”€â”€ OpenAI GPT-4 parsing
â”œâ”€â”€ Pattern matching fallback
â”œâ”€â”€ Confidence scoring
â””â”€â”€ Claude coaching response

/api/ai/parse-image        # Image analysis
â”œâ”€â”€ OCR for text in images
â”œâ”€â”€ AI food recognition
â””â”€â”€ Exercise form analysis

/api/activities/log        # Unified logging
â”œâ”€â”€ Save to activity_logs table
â”œâ”€â”€ Type-specific processing
â””â”€â”€ Real-time updates
```

### Database
```sql
-- Unified activity table
CREATE TABLE activity_logs (
  id UUID PRIMARY KEY,
  user_id UUID,
  type TEXT, -- 'food', 'weight', 'exercise', 'mood'
  raw_input TEXT,
  parsed_data JSONB,
  confidence FLOAT,
  auto_logged BOOLEAN,
  created_at TIMESTAMPTZ
);
```

## âœ… Success Criteria

The MVP is A+ when:
1. **Unified input works** for all three types (food, weight, exercise)
2. **Voice input** transcribes and parses correctly
3. **Photo input** recognizes food and logs it
4. **Auto-logging** works for high-confidence inputs
5. **Confirmation UI** appears for uncertain inputs
6. **Everything tested** end-to-end with real examples

## ðŸ“Š Testing Checklist

- [ ] Weight: "175" â†’ Confirmation â†’ Logs
- [ ] Weight: "weight 175" â†’ Auto-logs
- [ ] Food: "chicken salad" â†’ Auto-logs
- [ ] Food: Photo of meal â†’ Recognizes â†’ Logs
- [ ] Exercise: "ran 5k" â†’ Auto-logs
- [ ] Voice: Speak any above â†’ Transcribes â†’ Logs
- [ ] Mixed: "weight 175, ran 5k, ate healthy" â†’ Parses all three
- [ ] Uncertain: "did stuff" â†’ Asks for clarification

## ðŸš€ Deployment Plan

1. **Complete implementation** (4-6 hours)
2. **Test all scenarios** (2 hours)
3. **Deploy to production** (30 minutes)
4. **Monitor and iterate** based on user feedback

## ðŸŽ¯ The One Test That Matters

Open the app and say: **"Weight 175, ran 5k, had eggs for breakfast"**

If this logs all three activities correctly, we have achieved A+.

---

**This is the ONLY MVP documentation that matters. Everything else is archived.**